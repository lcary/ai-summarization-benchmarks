(Pdb) p shared.settings
{'dark_theme': True, 'show_controls': True, 'start_with': '', 'mode': 'instruct', 'chat_style': 'wpp', 'character': 'None', 'prompt-default': 'QA', 'prompt-notebook': 'QA', 'preset': 'simple-1', 'max_new_tokens': 200, 'max_new_tokens_min': 1, 'max_new_tokens_max': 4096, 'seed': -1, 'negative_prompt': '', 'truncation_length': 4096, 'truncation_length_min': 0, 'truncation_length_max': 16384, 'custom_stopping_strings': '', 'auto_max_new_tokens': False, 'max_tokens_second': 0, 'ban_eos_token': False, 'add_bos_token': True, 'skip_special_tokens': True, 'stream': True, 'name1': 'You', 'name2': 'Assistant', 'context': 'This is a conversation with your Assistant. It is a computer program designed to help you with various tasks such as answering questions, providing recommendations, and helping with decision making. You can ask it anything you want and it will do its best to give you accurate and relevant information.', 'greeting': '', 'instruction_template': 'Llama-v2', 'chat-instruct_command': 'Continue the chat dialogue below. Write a single reply for the character "<|character|>".\n\n<|prompt|>', 'autoload_model': False, 'default_extensions': ['gallery'], 'wbits': 'None', 'model_type': 'llama', 'groupsize': 'None', 'pre_layer': 0, 'n_gqa': 0, 'rms_norm_eps': 5e-06, 'rope_freq_base': 0}
(Pdb) p shared.model_config
OrderedDict([('.*', {'wbits': 'None', 'model_type': 'None', 'groupsize': 'None', 'pre_layer': 0, 'mode': 'chat', 'skip_special_tokens': True, 'custom_stopping_strings': '', 'truncation_length': 2048, 'n_gqa': 0, 'rms_norm_eps': 0, 'rope_freq_base': 0}), ('.*(llama|alpac|vicuna|guanaco|koala|llava|wizardlm|metharme|pygmalion-7b|wizard-mega|openbuddy|vigogne|h2ogpt-research|manticore)', {'model_type': 'llama'}), ('.*(opt-|opt_|opt1|opt3|optfor|galactica|galpaca|pygmalion-350m)', {'model_type': 'opt'}), ('.*(gpt-j|gptj|gpt4all-j|malion-6b|pygway|pygmalion-6b|dolly-v1)', {'model_type': 'gptj'}), ('.*(gpt-neox|koalpaca-polyglot|polyglot.*koalpaca|polyglot-ko|polyglot_ko|pythia|stablelm|incite|dolly-v2|polycoder|h2ogpt-oig|h2ogpt-oasst1|h2ogpt-gm)', {'model_type': 'gptneox'}), ('.*llama', {'model_type': 'llama'}), ('.*bloom', {'model_type': 'bloom'}), ('.*gpt2', {'model_type': 'gpt2'}), ('.*falcon', {'model_type': 'falcon'}), ('.*mpt', {'model_type': 'mpt'}), ('.*(starcoder|starchat)', {'model_type': 'starcoder'}), ('.*dolly-v2', {'model_type': 'dollyv2'}), ('.*replit', {'model_type': 'replit'}), ('llama-65b-gptq-3bit', {'groupsize': 'None'}), ('.*(4bit|int4)', {'wbits': 4}), ('.*(3bit|int3)', {'wbits': 3}), ('.*(-2bit|_2bit|int2-)', {'wbits': 2}), ('.*(-1bit|_1bit|int1-)', {'wbits': 1}), ('.*(8bit|int8)', {'wbits': 8}), ('.*(-7bit|_7bit|int7-)', {'wbits': 7}), ('.*(-6bit|_6bit|int6-)', {'wbits': 6}), ('.*(-5bit|_5bit|int5-)', {'wbits': 5}), ('.*(-gr32-|-32g-|groupsize32|-32g$)', {'groupsize': 32}), ('.*(-gr64-|-64g-|groupsize64|-64g$)', {'groupsize': 64}), ('.*(gr128|128g|groupsize128)', {'groupsize': 128}), ('.*(gr1024|1024g|groupsize1024)', {'groupsize': 1024}), ('.*(oasst|openassistant-|stablelm-7b-sft-v7-epoch-3)', {'mode': 'instruct', 'instruction_template': 'Open Assistant', 'skip_special_tokens': False}), ('(?!.*galactica)(?!.*reward).*openassistant', {'mode': 'instruct', 'instruction_template': 'Open Assistant', 'skip_special_tokens': False}), ('(?!.*v0)(?!.*1.1)(?!.*1_1)(?!.*stable)(?!.*chinese).*vicuna', {'mode': 'instruct', 'instruction_template': 'Vicuna-v0'}), ('.*vicuna.*v0', {'mode': 'instruct', 'instruction_template': 'Vicuna-v0'}), ('.*vicuna.*(1.1|1_1|1.3|1_3)', {'mode': 'instruct', 'instruction_template': 'Vicuna-v1.1'}), ('.*vicuna.*(1.5|1_5)', {'mode': 'instruct', 'instruction_template': 'Vicuna-v1.1', 'truncation_length': 4096, 'rms_norm_eps': 5e-06}), ('.*stable.*vicuna', {'mode': 'instruct', 'instruction_template': 'StableVicuna'}), ('(?!.*chat).*chinese-vicuna', {'mode': 'instruct', 'instruction_template': 'Alpaca'}), ('.*chinese-vicuna.*chat', {'mode': 'instruct', 'instruction_template': 'Chinese-Vicuna-Chat'}), ('.*alpaca', {'mode': 'instruct', 'instruction_template': 'Alpaca'}), ('.*alpaca-native-4bit', {'mode': 'instruct', 'instruction_template': 'Alpaca', 'wbits': 4, 'groupsize': 128}), ('.*galactica', {'skip_special_tokens': False}), ('.*dolly-v[0-9]-[0-9]*b', {'mode': 'instruct', 'instruction_template': 'Alpaca', 'skip_special_tokens': False, 'custom_stopping_strings': '"### End"'}), ('.*koala', {'mode': 'instruct', 'instruction_template': 'Koala'}), ('.*chatglm', {'mode': 'instruct', 'instruction_template': 'ChatGLM'}), ('.*metharme', {'mode': 'instruct', 'instruction_template': 'Metharme'}), ('.*llava', {'mode': 'instruct', 'model_type': 'llama', 'instruction_template': 'LLaVA', 'custom_stopping_strings': '"\\n###"'}), ('.*raven', {'mode': 'instruct', 'instruction_template': 'RWKV-Raven'}), ('.*ctx8192', {'truncation_length': 8192}), ('.*moss-moon.*sft', {'mode': 'instruct', 'instruction_template': 'MOSS'}), ('.*stablelm-tuned', {'mode': 'instruct', 'instruction_template': 'StableLM', 'truncation_length': 4096}), ('.*stablelm-base', {'truncation_length': 4096}), ('.*galactica.*finetuned', {'mode': 'instruct', 'instruction_template': 'Galactica Finetuned'}), ('.*galactica.*-v2', {'mode': 'instruct', 'instruction_template': 'Galactica v2'}), ('(?!.*finetuned)(?!.*-v2).*galactica', {'mode': 'instruct', 'instruction_template': 'Galactica'}), ('.*guanaco', {'mode': 'instruct', 'instruction_template': 'Guanaco non-chat'}), ('.*baize', {'mode': 'instruct', 'instruction_template': 'Baize'}), ('.*mpt-.*instruct', {'mode': 'instruct', 'instruction_template': 'Alpaca'}), ('.*mpt-.*chat', {'mode': 'instruct', 'instruction_template': 'MPT-Chat'}), ('(?!.*-flan-)(?!.*-t5-).*lamini-', {'mode': 'instruct', 'instruction_template': 'Alpaca'}), ('.*incite.*chat', {'mode': 'instruct', 'instruction_template': 'INCITE-Chat'}), ('.*incite.*instruct', {'mode': 'instruct', 'instruction_template': 'INCITE-Instruct'}), ('.*wizard.*mega', {'mode': 'instruct', 'instruction_template': 'Wizard-Mega', 'custom_stopping_strings': '"</s>"'}), ('.*ziya-', {'mode': 'instruct', 'instruction_template': 'Ziya'}), ('.*koalpaca', {'mode': 'instruct', 'instruction_template': 'KoAlpaca'}), ('.*openbuddy', {'mode': 'instruct', 'instruction_template': 'OpenBuddy'}), ('(?!.*chat).*vigogne', {'mode': 'instruct', 'instruction_template': 'Vigogne-Instruct'}), ('.*vigogne.*chat', {'mode': 'instruct', 'instruction_template': 'Vigogne-Chat'}), ('.*(llama-deus|supercot|llama-natural-instructions|open-llama-0.3t-7b-instruct-dolly-hhrlhf|open-llama-0.3t-7b-open-instruct)', {'mode': 'instruct', 'instruction_template': 'Alpaca'}), ('.*bactrian', {'mode': 'instruct', 'instruction_template': 'Bactrian'}), ('.*(h2ogpt-oig-|h2ogpt-oasst1-|h2ogpt-research-oasst1-)', {'mode': 'instruct', 'instruction_template': 'H2O-human_bot'}), ('.*h2ogpt-gm-', {'mode': 'instruct', 'instruction_template': 'H2O-prompt_answer'}), ('.*manticore', {'mode': 'instruct', 'instruction_template': 'Manticore Chat'}), ('.*bluemoonrp-(30|13)b', {'mode': 'instruct', 'instruction_template': 'Bluemoon', 'truncation_length': 4096}), ('.*Nous-Hermes-13b', {'mode': 'instruct', 'instruction_template': 'Alpaca'}), ('.*airoboros', {'mode': 'instruct', 'instruction_template': 'Vicuna-v1.1'}), ('.*airoboros.*1.2', {'mode': 'instruct', 'instruction_template': 'Airoboros-v1.2'}), ('.*alpa(cino|sta)', {'mode': 'instruct', 'instruction_template': 'Alpaca'}), ('.*hippogriff', {'mode': 'instruct', 'instruction_template': 'Hippogriff'}), ('.*lazarus', {'mode': 'instruct', 'instruction_template': 'Alpaca'}), ('.*guanaco-.*(7|13|33|65)b', {'mode': 'instruct', 'instruction_template': 'Guanaco'}), ('.*hypermantis', {'mode': 'instruct', 'instruction_template': 'Alpaca'}), ('.*open-llama-.*-open-instruct', {'mode': 'instruct', 'instruction_template': 'Alpaca'}), ('.*starcoder-gpteacher-code-instruct', {'mode': 'instruct', 'instruction_template': 'Alpaca'}), ('.*tulu', {'mode': 'instruct', 'instruction_template': 'Tulu'}), ('.*chronos', {'mode': 'instruct', 'instruction_template': 'Alpaca'}), ('.*samantha', {'mode': 'instruct', 'instruction_template': 'Samantha'}), ('.*wizardcoder', {'mode': 'instruct', 'instruction_template': 'Alpaca'}), ('.*starchat-beta', {'mode': 'instruct', 'instruction_template': 'Starchat-Beta', 'custom_stopping_strings': '"<|end|>"'}), ('.*minotaur', {'mode': 'instruct', 'instruction_template': 'Minotaur'}), ('.*minotaur-15b', {'truncation_length': 8192}), ('.*orca_mini', {'mode': 'instruct', 'instruction_template': 'Orca Mini'}), ('.*landmark', {'truncation_length': 8192}), ('.*superhot-8k', {'truncation_length': 8192}), ('.*xgen.*-inst', {'truncation_length': 8192, 'instruction_template': 'Vicuna-v0'}), ('.*(platypus|gplatty|superplatty)', {'mode': 'instruct', 'instruction_template': 'Alpaca'}), ('.*longchat', {'mode': 'instruct', 'instruction_template': 'Vicuna-v1.1'}), ('.*vicuna-33b', {'mode': 'instruct', 'instruction_template': 'Vicuna-v1.1'}), ('.*redmond-hermes-coder', {'mode': 'instruct', 'instruction_template': 'Alpaca', 'truncation_length': 8192}), ('.*wizardcoder-15b', {'mode': 'instruct', 'instruction_template': 'Alpaca', 'truncation_length': 8192}), ('.*wizardlm', {'mode': 'instruct', 'instruction_template': 'Vicuna-v1.1'}), ('.*godzilla', {'mode': 'instruct', 'instruction_template': 'Alpaca'}), ('.*llama-(2|v2)', {'truncation_length': 4096, 'rms_norm_eps': 5e-06}), ('.*llama-(2|v2).*chat', {'mode': 'instruct', 'instruction_template': 'Llama-v2'}), ('.*70b.*ggml.*\\.bin', {'n_gqa': 8}), ('.*newhope', {'mode': 'instruct', 'instruction_template': 'NewHope'}), ('.*stablebeluga2', {'mode': 'instruct', 'instruction_template': 'StableBeluga2', 'truncation_length': 4096, 'rms_norm_eps': 5e-06}), ('.*openchat', {'mode': 'instruct', 'instruction_template': 'OpenChat'}), ('.*falcon.*-instruct', {'mode': 'instruct'}), ('.*(openorca-platypus2)', {'mode': 'instruct', 'instruction_template': 'OpenOrca-Platypus2', 'custom_stopping_strings': '"### Instruction:", "### Response:"', 'rms_norm_eps': 5e-06}), ('.*codellama', {'rope_freq_base': 1000000}), ('.*codellama.*instruct', {'mode': 'instruct', 'instruction_template': 'Llama-v2'})])
(Pdb) p model_settings
{'wbits': 'None', 'model_type': 'llama', 'groupsize': 'None', 'pre_layer': 0, 'mode': 'instruct', 'skip_special_tokens': True, 'custom_stopping_strings': '', 'truncation_length': 4096, 'n_gqa': 0, 'rms_norm_eps': 5e-06, 'rope_freq_base': 0, 'instruction_template': 'Llama-v2'}
(Pdb) shared.model, shared.tokenizer
(LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
), LlamaTokenizer(name_or_path='models/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False))
(Pdb) p shared.args
Namespace(notebook=False, chat=False, multi_user=False, character=None, model='/content/text-generation-webui/models/Llama-2-7b-chat-hf', lora=None, model_dir='models/', lora_dir='loras/', model_menu=False, no_stream=False, settings='/content/settings.yaml', extensions=['gallery'], verbose=False, loader='Transformers', cpu=False, auto_devices=False, gpu_memory=None, cpu_memory=None, disk=False, disk_cache_dir='cache', load_in_8bit=False, bf16=False, no_cache=False, xformers=False, sdp_attention=False, trust_remote_code=False, load_in_4bit=False, compute_dtype='float16', quant_type='nf4', use_double_quant=False, threads=0, n_batch=512, no_mmap=False, low_vram=False, mlock=False, mul_mat_q=False, cache_capacity=None, n_gpu_layers=0, tensor_split=None, n_ctx=2048, llama_cpp_seed=0, n_gqa=0, rms_norm_eps=5e-06, wbits=0, model_type='llama', groupsize=-1, pre_layer=None, checkpoint=None, monkey_patch=False, triton=False, no_inject_fused_attention=False, no_inject_fused_mlp=False, no_use_cuda_fp16=False, desc_act=False, disable_exllama=False, gpu_split=None, max_seq_len=2048, cfg_cache=False, deepspeed=False, nvme_offload_dir=None, local_rank=0, rwkv_strategy=None, rwkv_cuda_on=False, alpha_value=1, rope_freq_base=0, compress_pos_emb=1, listen=False, listen_host=None, listen_port=None, share=True, auto_launch=False, gradio_auth=None, gradio_auth_path=None, ssl_keyfile=None, ssl_certfile=None, api=False, api_blocking_port=5000, api_streaming_port=5005, public_api=False, public_api_id=None, multimodal_pipeline=None)
(Pdb)